{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (20240930)\n",
      "Requirement already satisfied: numba in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from openai-whisper) (0.60.0)\n",
      "Requirement already satisfied: numpy in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from openai-whisper) (1.26.3)\n",
      "Requirement already satisfied: torch in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from openai-whisper) (2.6.0+cu124)\n",
      "Requirement already satisfied: tqdm in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from openai-whisper) (4.67.1)\n",
      "Requirement already satisfied: more-itertools in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from openai-whisper) (10.6.0)\n",
      "Requirement already satisfied: tiktoken in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from openai-whisper) (0.9.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from numba->openai-whisper) (0.43.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from tiktoken->openai-whisper) (2.32.3)\n",
      "Requirement already satisfied: filelock in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from torch->openai-whisper) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from torch->openai-whisper) (4.12.2)\n",
      "Requirement already satisfied: networkx in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from torch->openai-whisper) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from torch->openai-whisper) (3.1.4)\n",
      "Requirement already satisfied: fsspec in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from torch->openai-whisper) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from torch->openai-whisper) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: colorama in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from tqdm->openai-whisper) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Tried Whisper on apple silicon but doesn't play nice with Metal Performance Shader yet\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"small\", device=device) \n",
    "\n",
    "# using small instead of base for better accuracy\n",
    "# try medium or large for better accuracy but slower speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transcribe using whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio file...\n",
      "Transcript:\n",
      "\n",
      " Hospital arrangements were good, the beds were clean, the food was healthy and good, but the nurses were not on time and the medicines were not properly administered.\n"
     ]
    }
   ],
   "source": [
    "AUDIO_PATH = \"data/sample_audio4.mp3\"\n",
    "\n",
    "if os.path.exists(AUDIO_PATH):\n",
    "    print(\"Processing audio file...\")\n",
    "    result = model.transcribe(AUDIO_PATH)\n",
    "    print(\"Transcript:\\n\")\n",
    "    print(result[\"text\"])\n",
    "else:\n",
    "    print(\"‚ùå Audio file not found. Make sure it's at `data/sample_audio.mp3`.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "auto summarise using facebook/bart-large-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\miniconda\\envs\\careloop-ml\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n",
      "Your max_length is set to 150, but your input_length is only 35. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Hospital arrangements were good, the beds were clean, the food was healthy and good, but the nurses were not on time and the medicines were not properly administered.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def summarize_text(text):\n",
    "    try:\n",
    "        summary = summarizer(text, max_length=150, min_length=30, do_sample=False)[0][\"summary_text\"] # Adjust max_length and min_length as needed\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing text: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "patient_feedback_text = result[\"text\"]  \n",
    "summary = summarize_text(patient_feedback_text)\n",
    "\n",
    "if summary:\n",
    "    print(f\"Summary: {summary}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Concern Tagging using Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Concern Tags:\n",
      "- Food and Amenities: 0.34\n",
      "- Cleanliness: 0.18\n",
      "- Staff: 0.11\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "concern_categories = [\"Staff\", \"Billing\", \"Food and Amenities\", \"Cleanliness\", \"Post Discharge Care\", \"Communication\", \"Efficiency\", \"Comfort and Privacy\", \"Digital Experience\"]\n",
    "\n",
    "def tag_concerns_top_n(text, categories, top_n=3):\n",
    "    \"\"\"\n",
    "    Tags the input text with the top N most likely concern categories.\n",
    "\n",
    "    Args:\n",
    "        text (str): The patient feedback text.\n",
    "        categories (list): A list of potential concern categories.\n",
    "        top_n (int): The number of top categories to return (default is 3).\n",
    "\n",
    "    Returns:\n",
    "        list or None: A list of tuples, where each tuple contains (category, score),\n",
    "                     or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = classifier(text, candidate_labels=categories)\n",
    "        if result and result['labels'] and result['scores']:\n",
    "            top_results = list(zip(result['labels'][:top_n], result['scores'][:top_n]))\n",
    "            return top_results\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error tagging concern: {e}\")\n",
    "        return None\n",
    "\n",
    "# Your input text\n",
    "feedback_text = summary\n",
    "\n",
    "# Get the top 3 concern tags\n",
    "top_concerns = tag_concerns_top_n(feedback_text, concern_categories, top_n=3)\n",
    "\n",
    "if top_concerns:\n",
    "    print(f\"Top Concern Tags:\")\n",
    "    for tag, score in top_concerns:\n",
    "        print(f\"- {tag}: {score:.2f}\")\n",
    "else:\n",
    "    print(\"Could not determine concern tags.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis using nlptown/bert-base-multilingual-uncased-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: neutral (Score: 0.43)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "def analyze_sentiment_simplified(text):\n",
    "    \"\"\"\n",
    "    Analyzes the sentiment of the input text and maps it to positive, negative, or neutral.\n",
    "\n",
    "    Args:\n",
    "        text (str): The patient feedback text.\n",
    "\n",
    "    Returns:\n",
    "        tuple or None: A tuple containing the simplified sentiment label ('positive', 'negative', 'neutral')\n",
    "                       and the confidence score, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = sentiment_analyzer(text)[0]\n",
    "        star_label = result['label']\n",
    "        score = result['score']\n",
    "\n",
    "        if star_label in ['4 stars', '5 stars']:\n",
    "            sentiment = 'positive'\n",
    "        elif star_label in ['1 star', '2 stars']:\n",
    "            sentiment = 'negative'\n",
    "        else:  # '3 stars'\n",
    "            sentiment = 'neutral'\n",
    "\n",
    "        return sentiment, score\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Your input text\n",
    "feedback_text = summary\n",
    "sentiment_label, sentiment_score = analyze_sentiment_simplified(feedback_text)\n",
    "\n",
    "if sentiment_label:\n",
    "    print(f\"Sentiment: {sentiment_label} (Score: {sentiment_score:.2f})\")\n",
    "else:\n",
    "    print(\"Could not determine sentiment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transcription': ' Hospital arrangements were good, the beds were clean, the food was healthy and good, but the nurses were not on time and the medicines were not properly administered.', 'summary': 'Hospital arrangements were good, the beds were clean, the food was healthy and good, but the nurses were not on time and the medicines were not properly administered.', 'concern_tags': [{'tag': 'Food and Amenities', 'score': 0.34385946393013}, {'tag': 'Cleanliness', 'score': 0.1792309433221817}, {'tag': 'Staff', 'score': 0.11391789466142654}], 'sentiment': 'neutral', 'sentiment_score': 0.4261808395385742}\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'results', 'summary', 'top_concerns', 'sentiment', and 'sentiment_score' are already defined\n",
    "\n",
    "processed_feedback = {\n",
    "    \"transcription\": result[\"text\"] if \"text\" in result else None,\n",
    "    \"summary\": summary,\n",
    "    \"concern_tags\": [],\n",
    "    \"sentiment\": sentiment_label,\n",
    "    \"sentiment_score\": sentiment_score\n",
    "}\n",
    "\n",
    "# Iterate through the list of top concerns and add them to the dictionary\n",
    "if isinstance(top_concerns, list):\n",
    "    for tag, score in top_concerns:\n",
    "        processed_feedback[\"concern_tags\"].append({\"tag\": tag, \"score\": score})\n",
    "\n",
    "print(processed_feedback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "careloop-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
